{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4887a48",
   "metadata": {},
   "source": [
    "Data model summary & relationships (quick)\n",
    "\n",
    "        Customers (PK customer_id) — has zip prefix linking to GeoLocation.\n",
    "\n",
    "        GeoLocation (PK geolocation_zip_code_prefix) — latitude/longitude for zip areas.\n",
    "\n",
    "        Orders (PK order_id) — links to customer_id. Contains purchase/approved/delivery timestamps.\n",
    "\n",
    "        Order_Items — line items per order (order_id, product_id, seller_id).\n",
    "\n",
    "        Products (PK product_id) — product attributes; product_category_name links to Product_Category_Name_Translation.\n",
    "\n",
    "        Sellers (PK seller_id) — seller address zip links to GeoLocation.\n",
    "\n",
    "        Order_Payments — payment breakdown per order.\n",
    "\n",
    "        Order_Reviews — reviews against orders.\n",
    "\n",
    "        Leads_Qualified ↔ Leads_Closed — lead funnel (mql_id) with first-contact and won_date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ade09",
   "metadata": {},
   "source": [
    "A. Basic data quality & ETL checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb460c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  table_name  total_rows  missing_customer_id  missing_zip\n",
      "0  customers       99441                    0            0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "  'customers' AS table_name, \n",
    "  COUNT(*) AS total_rows,\n",
    "  SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS missing_customer_id,\n",
    "  SUM(CASE WHEN customer_zip_code_prefix IS NULL THEN 1 ELSE 0 END) AS missing_zip\n",
    "FROM customers;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cd5ad",
   "metadata": {},
   "source": [
    "B. Orders & fulfillment times (core for delays)\n",
    "        Goal: compute cycle times and identify slow steps.\n",
    "        Key metrics:\n",
    "\n",
    "        time_to_approve = order_approved_at - order_purchase_timestamp\n",
    "\n",
    "        time_to_ship = order_delivered_carrier_date - order_approved_at\n",
    "\n",
    "        time_to_customer = order_delivered_customer_date - order_delivered_carrier_date\n",
    "\n",
    "        lead_time_vs_estimate = order_delivered_customer_date - order_estimated_delivery_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  order_purchase_timestamp AS purchase_ts,\n",
    "  order_approved_at AS approved_ts,\n",
    "  order_delivered_carrier_date AS carrier_ts,\n",
    "  order_delivered_customer_date AS delivered_ts,\n",
    "\n",
    "  -- Hours to approve\n",
    "  (julianday(order_approved_at) - julianday(order_purchase_timestamp)) * 24 AS hours_to_approve,\n",
    "\n",
    "  -- Hours to ship\n",
    "  (julianday(order_delivered_carrier_date) - julianday(order_approved_at)) * 24 AS hours_to_ship,\n",
    "\n",
    "  -- Carrier to customer\n",
    "  (julianday(order_delivered_customer_date) - julianday(order_delivered_carrier_date)) * 24 AS hours_carrier_to_customer,\n",
    "\n",
    "  -- Late vs estimated\n",
    "  (julianday(order_delivered_customer_date) - julianday(order_estimated_delivery_date)) * 24 AS hours_late\n",
    "\n",
    "FROM orders\n",
    "WHERE order_delivered_customer_date IS NOT NULL\n",
    "LIMIT 50;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"delivery_analysis.csv\", index=False)\n",
    "\n",
    "print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28dd9e9",
   "metadata": {},
   "source": [
    "Interpretation: compute distributions (median, 90th percentile) for each metric; 90th percentile pinpoints tail risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832172a1",
   "metadata": {},
   "source": [
    "C. Seller & product-level fulfillment performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "WITH delivery_diff AS (\n",
    "    SELECT \n",
    "        oi.seller_id,\n",
    "        p.product_category_name,\n",
    "        o.order_id,\n",
    "        (julianday(o.order_delivered_customer_date) - julianday(o.order_estimated_delivery_date)) * 24 AS delay_hours\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "),\n",
    "median_calc AS (\n",
    "    SELECT \n",
    "        seller_id,\n",
    "        product_category_name,\n",
    "        delay_hours,\n",
    "        ROW_NUMBER() OVER (PARTITION BY seller_id, product_category_name ORDER BY delay_hours) AS rn,\n",
    "        COUNT(*) OVER (PARTITION BY seller_id, product_category_name) AS cnt,\n",
    "        order_id\n",
    "    FROM delivery_diff\n",
    ")\n",
    "SELECT \n",
    "    seller_id,\n",
    "    product_category_name,\n",
    "    COUNT(DISTINCT order_id) AS orders_count,\n",
    "    AVG(delay_hours) AS avg_delay_hours,\n",
    "    -- crude median approximation\n",
    "    AVG(delay_hours) FILTER (\n",
    "        WHERE rn IN ((cnt + 1)/2, (cnt + 2)/2)\n",
    "    ) AS median_delay_hours\n",
    "FROM median_calc\n",
    "GROUP BY seller_id, product_category_name\n",
    "ORDER BY median_delay_hours DESC\n",
    "LIMIT 50;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Seller & product-level fulfillment performance.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637455f",
   "metadata": {},
   "source": [
    "Interpretation: sellers/categories with high median/mean delays are candidates for intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194036b5",
   "metadata": {},
   "source": [
    "D. Delivery bottlenecks by geography & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "WITH last_mile AS (\n",
    "    SELECT \n",
    "        c.customer_zip_code_prefix,\n",
    "        (julianday(o.order_delivered_customer_date) - julianday(o.order_delivered_carrier_date)) * 24 AS last_mile_hours\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        customer_zip_code_prefix,\n",
    "        last_mile_hours,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_zip_code_prefix ORDER BY last_mile_hours) AS rn,\n",
    "        COUNT(*) OVER (PARTITION BY customer_zip_code_prefix) AS cnt\n",
    "    FROM last_mile\n",
    ")\n",
    "SELECT \n",
    "    customer_zip_code_prefix,\n",
    "    COUNT(*) AS orders_count,\n",
    "    AVG(last_mile_hours) AS avg_last_mile_hours,\n",
    "    -- crude 90th percentile: pick row at position ceil(0.9 * cnt)\n",
    "    MAX(last_mile_hours) FILTER (\n",
    "        WHERE rn = CAST(0.9 * cnt AS INT)\n",
    "              OR rn = CAST(0.9 * cnt + 1 AS INT)\n",
    "    ) AS p90_last_mile_hours\n",
    "FROM ranked\n",
    "GROUP BY customer_zip_code_prefix\n",
    "ORDER BY p90_last_mile_hours DESC\n",
    "LIMIT 50;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Delivery bottlenecks by geography & time.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052f47a",
   "metadata": {},
   "source": [
    "Interpretation: zip prefixes with high p90 indicate problematic delivery areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598e96c",
   "metadata": {},
   "source": [
    "This gives you:\n",
    "\n",
    "        orders_count\n",
    "\n",
    "        avg_last_mile_hours\n",
    "\n",
    "        Approximate p90_last_mile_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41887d4",
   "metadata": {},
   "source": [
    "E. Revenue-at-risk from late deliveries / predicted churn\n",
    "\n",
    "Goal: measure revenue exposure and link late deliveries to churn (or negative reviews).\n",
    "Steps:\n",
    "\n",
    "        Flag orders that were delivered later than estimated.\n",
    "\n",
    "        Aggregate payment_value per customer and segment customers with % of late orders.\n",
    "\n",
    "        Model churn probability using logistic regression with features: late_delivery_rate, avg_delivery_time, avg_review_score, order_freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "WITH payments AS (\n",
    "    SELECT \n",
    "        order_id,\n",
    "        SUM(payment_value) AS total_payment\n",
    "    FROM order_payments\n",
    "    GROUP BY order_id\n",
    "),\n",
    "orders_with_flag AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        CASE \n",
    "            WHEN o.order_delivered_customer_date > o.order_estimated_delivery_date THEN 1\n",
    "            ELSE 0\n",
    "        END AS late_flag\n",
    "    FROM orders o\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "),\n",
    "reviews AS (\n",
    "    SELECT \n",
    "        o.customer_id,\n",
    "        AVG(r.review_score) AS avg_review\n",
    "    FROM orders o\n",
    "    JOIN order_reviews r ON o.order_id = r.order_id\n",
    "    GROUP BY o.customer_id\n",
    ")\n",
    "SELECT \n",
    "    o.customer_id,\n",
    "    COUNT(DISTINCT o.order_id) AS total_orders,\n",
    "    SUM(o.late_flag) AS late_orders,\n",
    "    SUM(p.total_payment) AS rev,\n",
    "    r.avg_review,\n",
    "    CAST(SUM(o.late_flag) AS FLOAT) / COUNT(DISTINCT o.order_id) AS late_rate\n",
    "FROM orders_with_flag o\n",
    "JOIN payments p ON o.order_id = p.order_id\n",
    "LEFT JOIN reviews r ON o.customer_id = r.customer_id\n",
    "GROUP BY o.customer_id\n",
    "ORDER BY rev DESC\n",
    "LIMIT 50;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Revenue-at-risk from late deliveries.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699924b",
   "metadata": {},
   "source": [
    "Interpretation: customers with high late_rate & falling review scores are retention targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db7ba0",
   "metadata": {},
   "source": [
    "3) Analyze delivery delays & bottlenecks — method & actionable steps\n",
    "            Approach — data-driven layered analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca24021",
   "metadata": {},
   "source": [
    "Compute end-to-end times (purchase→approval, approval→carrier pickup, carrier→customer). Use medians and high percentiles (75th, 90th) to find tail problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f5529c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   median_purchase_to_approve  median_approve_to_carrier  \\\n",
      "0                    0.343333                  43.575417   \n",
      "\n",
      "   median_carrier_to_customer  p75_purchase_to_approve  \\\n",
      "0                  170.394306                14.513889   \n",
      "\n",
      "   p75_approve_to_carrier  p75_carrier_to_customer  p90_purchase_to_approve  \\\n",
      "0               85.798333               288.697222                34.591944   \n",
      "\n",
      "   p90_approve_to_carrier  p90_carrier_to_customer  \n",
      "0                143.7625               453.569167  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "WITH diffs AS (\n",
    "    SELECT \n",
    "        order_id,\n",
    "        -- convert julianday diffs (days) to hours (* 24)\n",
    "        (julianday(order_approved_at) - julianday(order_purchase_timestamp)) * 24 AS purchase_to_approve,\n",
    "        (julianday(order_delivered_carrier_date) - julianday(order_approved_at)) * 24 AS approve_to_carrier,\n",
    "        (julianday(order_delivered_customer_date) - julianday(order_delivered_carrier_date)) * 24 AS carrier_to_customer\n",
    "    FROM orders\n",
    "    WHERE order_delivered_customer_date IS NOT NULL\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT \n",
    "        order_id,\n",
    "        purchase_to_approve,\n",
    "        approve_to_carrier,\n",
    "        carrier_to_customer,\n",
    "        ROW_NUMBER() OVER (ORDER BY purchase_to_approve) AS rn_p2a,\n",
    "        ROW_NUMBER() OVER (ORDER BY approve_to_carrier) AS rn_a2c,\n",
    "        ROW_NUMBER() OVER (ORDER BY carrier_to_customer) AS rn_c2cust,\n",
    "        COUNT(*) OVER () AS cnt\n",
    "    FROM diffs\n",
    ")\n",
    "SELECT \n",
    "    -- median = middle row(s)\n",
    "    AVG(purchase_to_approve) FILTER (\n",
    "        WHERE rn_p2a IN ((cnt + 1)/2, (cnt + 2)/2)\n",
    "    ) AS median_purchase_to_approve,\n",
    "    AVG(approve_to_carrier) FILTER (\n",
    "        WHERE rn_a2c IN ((cnt + 1)/2, (cnt + 2)/2)\n",
    "    ) AS median_approve_to_carrier,\n",
    "    AVG(carrier_to_customer) FILTER (\n",
    "        WHERE rn_c2cust IN ((cnt + 1)/2, (cnt + 2)/2)\n",
    "    ) AS median_carrier_to_customer,\n",
    "\n",
    "    -- 75th percentile\n",
    "    MAX(purchase_to_approve) FILTER (WHERE rn_p2a = CAST(0.75 * cnt AS INT)) AS p75_purchase_to_approve,\n",
    "    MAX(approve_to_carrier) FILTER (WHERE rn_a2c = CAST(0.75 * cnt AS INT)) AS p75_approve_to_carrier,\n",
    "    MAX(carrier_to_customer) FILTER (WHERE rn_c2cust = CAST(0.75 * cnt AS INT)) AS p75_carrier_to_customer,\n",
    "\n",
    "    -- 90th percentile\n",
    "    MAX(purchase_to_approve) FILTER (WHERE rn_p2a = CAST(0.90 * cnt AS INT)) AS p90_purchase_to_approve,\n",
    "    MAX(approve_to_carrier) FILTER (WHERE rn_a2c = CAST(0.90 * cnt AS INT)) AS p90_approve_to_carrier,\n",
    "    MAX(carrier_to_customer) FILTER (WHERE rn_c2cust = CAST(0.90 * cnt AS INT)) AS p90_carrier_to_customer\n",
    "FROM ranked;\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"end-to-end times (purchase→approval, approval→carrier pickup, carrier→customer).csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5d340",
   "metadata": {},
   "source": [
    "This will give you a single row summary with medians, 75th, and 90th percentile values for each segment:\n",
    "\n",
    "        purchase → approval\n",
    "\n",
    "        approval → carrier\n",
    "\n",
    "        carrier → customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03e967",
   "metadata": {},
   "source": [
    "Segment by dimension: seller_id, product_category, customer_zip_prefix, weekday/time-of-day, seasonal windows, payment_type, shipping_limit_date vs actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        oi.seller_id,\n",
    "        p.product_category_name,\n",
    "        c.customer_zip_code_prefix,\n",
    "        op.payment_type,\n",
    "        -- delivery times in hours\n",
    "        (julianday(o.order_approved_at) - julianday(o.order_purchase_timestamp)) * 24 AS purchase_to_approve,\n",
    "        (julianday(o.order_delivered_carrier_date) - julianday(o.order_approved_at)) * 24 AS approve_to_carrier,\n",
    "        (julianday(o.order_delivered_customer_date) - julianday(o.order_delivered_carrier_date)) * 24 AS carrier_to_customer,\n",
    "        (julianday(o.order_delivered_customer_date) - julianday(o.order_estimated_delivery_date)) * 24 AS late_by_hours,\n",
    "        strftime('%w', o.order_purchase_timestamp) AS purchase_weekday, -- 0=Sunday\n",
    "        strftime('%H', o.order_purchase_timestamp) AS purchase_hour,\n",
    "        strftime('%m', o.order_purchase_timestamp) AS purchase_month\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN products p ON oi.product_id = p.product_id\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    JOIN order_payments op ON o.order_id = op.order_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    seller_id,\n",
    "    product_category_name,\n",
    "    customer_zip_code_prefix,\n",
    "    payment_type,\n",
    "    purchase_weekday,\n",
    "    purchase_hour,\n",
    "    purchase_month,\n",
    "    COUNT(DISTINCT order_id) AS total_orders,\n",
    "    AVG(purchase_to_approve) AS avg_purchase_to_approve,\n",
    "    AVG(approve_to_carrier) AS avg_approve_to_carrier,\n",
    "    AVG(carrier_to_customer) AS avg_carrier_to_customer,\n",
    "    AVG(late_by_hours) AS avg_lateness,\n",
    "    SUM(CASE WHEN late_by_hours > 0 THEN 1 ELSE 0 END) * 1.0 / COUNT(*) AS late_rate\n",
    "FROM base\n",
    "GROUP BY \n",
    "    seller_id,\n",
    "    product_category_name,\n",
    "    customer_zip_code_prefix,\n",
    "    payment_type,\n",
    "    purchase_weekday,\n",
    "    purchase_hour,\n",
    "    purchase_month\n",
    "ORDER BY avg_lateness DESC\n",
    "LIMIT 100;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Segment by dimension.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f40cda",
   "metadata": {},
   "source": [
    "What this gives you\n",
    "\n",
    "Dimensions (segmentation):\n",
    "\n",
    "        seller_id\n",
    "\n",
    "        product_category_name\n",
    "\n",
    "        customer_zip_code_prefix\n",
    "\n",
    "        payment_type\n",
    "\n",
    "        purchase_weekday (day of week)\n",
    "\n",
    "        purchase_hour (time of day)\n",
    "\n",
    "        purchase_month (seasonality)\n",
    "\n",
    "        Metrics:\n",
    "\n",
    "        Avg time from purchase → approval, approval → carrier, carrier → customer\n",
    "\n",
    "        Avg lateness vs. estimated\n",
    "\n",
    "        % of late deliveries (late_rate)\n",
    "\n",
    "        Avg allowed shipping window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1af19f",
   "metadata": {},
   "source": [
    "Root-cause slicing:\n",
    "\n",
    "    If time_to_approve high → internal processing issue (fraud checks, payment verification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ea51824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  payment_type  orders  avg_approve_min  min_approve  max_approve\n",
      "0       boleto   19754          1986.99          0.0      87051.0\n",
      "1   debit_card    1529           572.01          0.0      40564.0\n",
      "2      voucher    5689           517.62          0.0      17118.0\n",
      "3  credit_card   76739           275.41          0.0     270550.0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "-- Time from purchase to approval by payment type\n",
    "WITH approve_times AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        CAST((julianday(o.order_approved_at) - julianday(o.order_purchase_timestamp)) * 24 * 60 AS INTEGER) AS time_to_approve_min,\n",
    "        p.payment_type\n",
    "    FROM orders o\n",
    "    JOIN order_payments p ON o.order_id = p.order_id\n",
    "    WHERE o.order_approved_at IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    payment_type,\n",
    "    COUNT(*) AS orders,\n",
    "    ROUND(AVG(time_to_approve_min), 2) AS avg_approve_min,\n",
    "    ROUND(MIN(time_to_approve_min), 2) AS min_approve,\n",
    "    ROUND(MAX(time_to_approve_min), 2) AS max_approve\n",
    "FROM approve_times\n",
    "GROUP BY payment_type\n",
    "ORDER BY avg_approve_min DESC;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(\"Segment by dimension.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509161f",
   "metadata": {},
   "source": [
    "Helps spot if payment verification or fraud checks are causing slow approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5f293",
   "metadata": {},
   "source": [
    "Root Cause Slicing:\n",
    "\n",
    "        Seller Fulfillment Issue (time_to_ship high) -- Time from approval to carrier pickup by seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6425f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_orders  avg_ship_hours  min_ship  max_ship\n",
      "0         97644           66.83   -4109.0    3018.0\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "-- Time from approval to carrier pickup (per order)\n",
    "WITH ship_times AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        CAST((julianday(o.order_delivered_carrier_date) - julianday(o.order_approved_at)) * 24 AS INTEGER) AS time_to_ship_hours\n",
    "    FROM orders o\n",
    "    WHERE o.order_delivered_carrier_date IS NOT NULL\n",
    "      AND o.order_approved_at IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    COUNT(*) AS total_orders,\n",
    "    ROUND(AVG(time_to_ship_hours), 2) AS avg_ship_hours,\n",
    "    ROUND(MIN(time_to_ship_hours), 2) AS min_ship,\n",
    "    ROUND(MAX(time_to_ship_hours), 2) AS max_ship\n",
    "FROM ship_times;\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(\"Segment by dimension.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474561a0",
   "metadata": {},
   "source": [
    "Flags seller stockouts, packaging delays, or late handovers to carrier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6996824d",
   "metadata": {},
   "source": [
    "Root Cause Slicing:\n",
    "\n",
    "        Last-Mile Carrier Issue (time_carrier_to_customer high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b15da61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    customer_zip_code_prefix  orders  avg_last_mile_hours  min_last_mile  \\\n",
      "0                      64013       1               4353.0         4353.0   \n",
      "1                      69308       1               4059.0         4059.0   \n",
      "2                      29724       1               3502.0         3502.0   \n",
      "3                      65735       2               2503.5         1097.0   \n",
      "4                      61663       1               2393.0         2393.0   \n",
      "5                      87895       1               2217.0         2217.0   \n",
      "6                      29278       4               1953.0          161.0   \n",
      "7                      28155       1               1855.0         1855.0   \n",
      "8                      60865       2               1779.5          218.0   \n",
      "9                      59970       1               1724.0         1724.0   \n",
      "10                     48130       2               1708.5         1664.0   \n",
      "11                     69316       1               1706.0         1706.0   \n",
      "12                      2755       1               1702.0         1702.0   \n",
      "13                     44135       1               1678.0         1678.0   \n",
      "14                     87518       1               1677.0         1677.0   \n",
      "15                     60511       2               1606.0         1347.0   \n",
      "16                     48906       2               1573.5          173.0   \n",
      "17                     75220       2               1559.5          170.0   \n",
      "18                     25907       1               1531.0         1531.0   \n",
      "19                     56485       1               1465.0         1465.0   \n",
      "\n",
      "    max_last_mile  \n",
      "0          4353.0  \n",
      "1          4059.0  \n",
      "2          3502.0  \n",
      "3          3910.0  \n",
      "4          2393.0  \n",
      "5          2217.0  \n",
      "6          3239.0  \n",
      "7          1855.0  \n",
      "8          3341.0  \n",
      "9          1724.0  \n",
      "10         1753.0  \n",
      "11         1706.0  \n",
      "12         1702.0  \n",
      "13         1678.0  \n",
      "14         1677.0  \n",
      "15         1865.0  \n",
      "16         2974.0  \n",
      "17         2949.0  \n",
      "18         1531.0  \n",
      "19         1465.0  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "-- Time from carrier pickup to delivery, segmented by customer zip prefix\n",
    "WITH last_mile AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        c.customer_zip_code_prefix,\n",
    "        CAST((julianday(o.order_delivered_customer_date) - julianday(o.order_delivered_carrier_date)) * 24 AS INTEGER) AS time_carrier_to_customer_hours\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "      AND o.order_delivered_carrier_date IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    customer_zip_code_prefix,\n",
    "    COUNT(*) AS orders,\n",
    "    ROUND(AVG(time_carrier_to_customer_hours), 2) AS avg_last_mile_hours,\n",
    "    ROUND(MIN(time_carrier_to_customer_hours), 2) AS min_last_mile,\n",
    "    ROUND(MAX(time_carrier_to_customer_hours), 2) AS max_last_mile\n",
    "FROM last_mile\n",
    "GROUP BY customer_zip_code_prefix\n",
    "ORDER BY avg_last_mile_hours DESC\n",
    "LIMIT 20;\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(\"Segment by dimension.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffe9893",
   "metadata": {},
   "source": [
    "Helps identify last-mile carrier issues or address accuracy problems, clustered by ZIP code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd71be",
   "metadata": {},
   "source": [
    "Correlate with reviews and refunds: \n",
    "\n",
    "        late deliveries → negative reviews and refunds. \n",
    "        Use uplift tests: compare retention/repeat purchase rates for customers with late deliveries vs without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8775378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   late_flag  customers  avg_review  total_negative_reviews  total_refunds  \\\n",
      "0          0      88649        4.29                   15276              0   \n",
      "1          1       7827        2.57                    5036              0   \n",
      "\n",
      "   repeat_rate  \n",
      "0          0.0  \n",
      "1          0.0  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "WITH orders_flagged AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        CASE \n",
    "            WHEN o.order_delivered_customer_date > o.order_estimated_delivery_date THEN 1\n",
    "            ELSE 0\n",
    "        END AS late_flag\n",
    "    FROM orders o\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "),\n",
    "reviews_agg AS (\n",
    "    SELECT \n",
    "        o.customer_id,\n",
    "        AVG(r.review_score) AS avg_review,\n",
    "        SUM(CASE WHEN r.review_score <= 3 THEN 1 ELSE 0 END) AS negative_reviews\n",
    "    FROM orders_flagged o\n",
    "    LEFT JOIN order_reviews r ON o.order_id = r.order_id\n",
    "    GROUP BY o.customer_id\n",
    "),\n",
    "refunds_agg AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        SUM(CASE WHEN order_status = 'refunded' THEN 1 ELSE 0 END) AS refund_count\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    "),\n",
    "repeat_agg AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(DISTINCT order_id) AS total_orders,\n",
    "        CASE WHEN COUNT(DISTINCT order_id) > 1 THEN 1 ELSE 0 END AS repeat_customer\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT \n",
    "    o.late_flag,\n",
    "    COUNT(DISTINCT o.customer_id) AS customers,\n",
    "    ROUND(AVG(r.avg_review),2) AS avg_review,\n",
    "    SUM(r.negative_reviews) AS total_negative_reviews,\n",
    "    SUM(f.refund_count) AS total_refunds,\n",
    "    SUM(rep.repeat_customer) * 1.0 / COUNT(*) AS repeat_rate\n",
    "FROM orders_flagged o\n",
    "LEFT JOIN reviews_agg r ON o.customer_id = r.customer_id\n",
    "LEFT JOIN refunds_agg f ON o.customer_id = f.customer_id\n",
    "LEFT JOIN repeat_agg rep ON o.customer_id = rep.customer_id\n",
    "GROUP BY o.late_flag;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(\"Segment by dimension.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f0048",
   "metadata": {},
   "source": [
    "What this does\n",
    "\n",
    "        late_flag = 1 → late deliveries\n",
    "\n",
    "        late_flag = 0 → on-time deliveries\n",
    "\n",
    "        Computes per-group metrics:\n",
    "\n",
    "        Avg review score\n",
    "\n",
    "        Total negative reviews (score <= 3)\n",
    "\n",
    "        Total refunds\n",
    "\n",
    "        Repeat purchase rate (percentage of customers with >1 order)\n",
    "\n",
    "This lets you perform a simple uplift analysis: compare customers with late vs on-time deliveries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09bb7e",
   "metadata": {},
   "source": [
    "Visual diagnostics: heatmap of average delivery time by origin zip → destination zip (or seller zip → customer zip). Choropleth by state/zip p90 delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86443de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Connect to SQLite DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "# --- 1. Heatmap: Average delivery time by seller zip → customer zip ---\n",
    "query_heatmap = \"\"\"\n",
    "WITH delivery_times AS (\n",
    "    SELECT \n",
    "        s.seller_zip_code_prefix AS origin_zip,\n",
    "        c.customer_zip_code_prefix AS dest_zip,\n",
    "        (julianday(o.order_delivered_customer_date) - julianday(o.order_approved_at)) * 24 AS delivery_hours\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    origin_zip,\n",
    "    dest_zip,\n",
    "    ROUND(AVG(delivery_hours),2) AS avg_delivery_hours,\n",
    "    COUNT(*) AS orders_count\n",
    "FROM delivery_times\n",
    "GROUP BY origin_zip, dest_zip\n",
    "LIMIT 5000;\n",
    "\"\"\"\n",
    "df_heatmap = pd.read_sql_query(query_heatmap, conn)\n",
    "\n",
    "# Pivot for heatmap\n",
    "heatmap_matrix = df_heatmap.pivot(index='origin_zip', columns='dest_zip', values='avg_delivery_hours')\n",
    "\n",
    "# Plot heatmap\n",
    "fig_heatmap = px.imshow(\n",
    "    heatmap_matrix,\n",
    "    labels=dict(x=\"Customer Zip\", y=\"Seller Zip\", color=\"Avg Delivery Hours\"),\n",
    "    x=heatmap_matrix.columns,\n",
    "    y=heatmap_matrix.index,\n",
    "    color_continuous_scale=\"Viridis\"\n",
    ")\n",
    "fig_heatmap.update_layout(title=\"Avg Delivery Time: Seller Zip → Customer Zip\")\n",
    "fig_heatmap.write_html(\"heatmap.html\")\n",
    "\n",
    "# --- 2. Choropleth: p90 delivery delays by state ---\n",
    "query_state = \"\"\"\n",
    "WITH delays AS (\n",
    "    SELECT \n",
    "        c.customer_state,\n",
    "        (julianday(o.order_delivered_customer_date) - julianday(o.order_estimated_delivery_date)) * 24 AS late_hours\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    ")\n",
    "SELECT customer_state, late_hours\n",
    "FROM delays;\n",
    "\"\"\"\n",
    "df_state = pd.read_sql_query(query_state, conn)\n",
    "\n",
    "# Compute p90 per state\n",
    "df_p90 = df_state.groupby('customer_state')['late_hours'].apply(lambda x: np.percentile(x, 90)).reset_index()\n",
    "df_p90.rename(columns={'late_hours': 'p90_delay_hours'}, inplace=True)\n",
    "\n",
    "# Plot choropleth (US state codes)\n",
    "fig_choropleth = px.choropleth(\n",
    "    df_p90,\n",
    "    locations='customer_state',\n",
    "    locationmode=\"USA-states\",\n",
    "    color='p90_delay_hours',\n",
    "    color_continuous_scale=\"Reds\",\n",
    "    scope=\"usa\",\n",
    "    labels={'p90_delay_hours': 'P90 Delay Hours'}\n",
    ")\n",
    "fig_choropleth.update_layout(title=\"P90 Delivery Delay by State\")\n",
    "fig_choropleth.write_html(\"choropleth.html\")\n",
    "\n",
    "# Close DB connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979bad9",
   "metadata": {},
   "source": [
    "What this does:\n",
    "\n",
    "        Heatmap: Shows which seller→customer zip combinations are slowest.\n",
    "\n",
    "        Choropleth: Highlights states with the worst 90th percentile delivery delays.\n",
    "\n",
    "        Can easily be exported or embedded in a Power BI / web dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b09bd9",
   "metadata": {},
   "source": [
    "Experimentation: A/B test faster carriers or prioritized handling for specific sellers/segments and measure change in delivery times and NPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ff3f747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  experiment_group  total_orders  avg_time_to_ship  min_time_to_ship  \\\n",
      "0  priority_seller        110196            288.82           -167.75   \n",
      "\n",
      "   max_time_to_ship  avg_late_hours  \n",
      "0           5004.02          -272.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DB\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "-- Define experiment groups\n",
    "WITH experiment AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        oi.seller_id,\n",
    "        o.order_delivered_customer_date,\n",
    "        o.order_estimated_delivery_date,\n",
    "        o.order_approved_at,\n",
    "        o.order_delivered_carrier_date,\n",
    "        CASE \n",
    "            WHEN oi.seller_id IN (SELECT seller_id FROM sellers) THEN 'priority_seller'\n",
    "            ELSE 'control'\n",
    "        END AS experiment_group\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "),\n",
    "\n",
    "-- Compute delivery times\n",
    "delivery_metrics AS (\n",
    "    SELECT \n",
    "        experiment_group,\n",
    "    \n",
    "        (julianday(order_delivered_customer_date) - julianday(order_approved_at)) * 24 AS time_to_ship_hours,\n",
    "        (julianday(order_delivered_customer_date) - julianday(order_estimated_delivery_date)) * 24 AS late_hours\n",
    "    FROM experiment\n",
    ")\n",
    "\n",
    "-- Aggregate metrics by experiment group\n",
    "SELECT \n",
    "    experiment_group,\n",
    "    COUNT(*) AS total_orders,\n",
    "    ROUND(AVG(time_to_ship_hours),2) AS avg_time_to_ship,\n",
    "    ROUND(MIN(time_to_ship_hours),2) AS min_time_to_ship,\n",
    "    ROUND(MAX(time_to_ship_hours),2) AS max_time_to_ship,\n",
    "    ROUND(AVG(late_hours),2) AS avg_late_hours\n",
    "FROM delivery_metrics\n",
    "GROUP BY experiment_group;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(\"Segment by dimension.csv\", index=False)\n",
    "# print(\"✅ Results exported to delivery_analysis.csv\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6e852",
   "metadata": {},
   "source": [
    "Actionable interventions:\n",
    "\n",
    "        Automate approval step where safe, reduce manual review burden.\n",
    "\n",
    "        Enforce SLA with sellers: auto-suspend sellers with persistent shipping lateness; provide training or incentives.\n",
    "\n",
    "        Re-route orders to alternate sellers or warehouses when seller's stock shows frequent delays.\n",
    "\n",
    "        Work with carriers on high-delay zip codes — increase pickups/frequency or change carrier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04af9da",
   "metadata": {},
   "source": [
    "5) Forecast demand to reduce stockouts — approach & features\n",
    "\n",
    "        Objectives: forecast product-level demand (daily/weekly) to:\n",
    "\n",
    "                Set safety stock and reorder points.\n",
    "\n",
    "                Drive warehouse replenishment planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095c8ac2",
   "metadata": {},
   "source": [
    "Complete Pipeline for product-level demand forecasting, including feature engineering and aggregation steps, suitable for driving safety stock and reorder point calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e358affe",
   "metadata": {},
   "source": [
    "1. Aggregate daily sales per product per warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cd018f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_sales AS\n",
    "WITH order_dates AS (\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.order_purchase_timestamp,\n",
    "        o.customer_id\n",
    "    FROM orders o\n",
    "    WHERE o.order_status = 'delivered'\n",
    "),\n",
    "product_sales AS (\n",
    "    SELECT \n",
    "        oi.product_id,\n",
    "        s.seller_id AS warehouse_id,\n",
    "        DATE(o.order_purchase_timestamp) AS sales_date,\n",
    "        COUNT(oi.order_item_id) AS units_sold,\n",
    "        SUM(oi.price) AS revenue\n",
    "    FROM order_items oi\n",
    "    JOIN order_dates o ON oi.order_id = o.order_id\n",
    "    JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "    GROUP BY oi.product_id, warehouse_id, sales_date\n",
    ")\n",
    "SELECT *\n",
    "FROM product_sales;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "cursor.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM daily_sales\", conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Daily_Sales.csv\", index=False)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b347fb",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "        Aggregates daily units sold and revenue per product.\n",
    "\n",
    "        Missing dates (no sales) should be filled with zeros later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4071767",
   "metadata": {},
   "source": [
    "2️⃣ Add time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a860b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "-- Add day-of-week, month, week number\n",
    "CREATE TABLE IF NOT EXISTS daily_sales_features AS\n",
    "SELECT \n",
    "    ds.*,\n",
    "    STRFTIME('%w', ds.sales_date) AS day_of_week,  -- 0=Sunday\n",
    "    STRFTIME('%m', ds.sales_date) AS month,\n",
    "    STRFTIME('%W', ds.sales_date) AS week_of_year\n",
    "FROM daily_sales ds;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "cursor.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19089a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM daily_sales_features\", conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Daily_Sales_features.csv\", index=False)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c9a1b",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "        Helps capture seasonality and weekly patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225da6f",
   "metadata": {},
   "source": [
    "3️⃣ Add lag features and rolling statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c949b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "-- Lag-1 units sold\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS daily_sales_lag AS\n",
    "SELECT \n",
    "    f1.product_id,\n",
    "    f1.warehouse_id,\n",
    "    f1.sales_date,\n",
    "    f1.units_sold,\n",
    "    f1.revenue,\n",
    "    COALESCE(f2.units_sold,0) AS lag_1_units\n",
    "FROM daily_sales_features f1\n",
    "LEFT JOIN daily_sales_features f2\n",
    "  ON f1.product_id = f2.product_id\n",
    "  AND f1.warehouse_id = f2.warehouse_id\n",
    "  AND DATE(f2.sales_date, '+1 day') = f1.sales_date;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "cursor.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15889db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM daily_sales_lag\", conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"Daily_Sales_lag.csv\", index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5a2af",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "        lag_1_units is the number of units sold the previous day.\n",
    "\n",
    "        You can extend this pattern for lag_7, rolling 7-day average, rolling 30-day std using similar joins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8202b6a",
   "metadata": {},
   "source": [
    "4️⃣ Include promotional or B2B signals:\n",
    "\n",
    "        If using Leads_Closed or Leads_Qualified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49611c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "-- Example: flag products promoted via leads\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS sales_with_promotions AS\n",
    "SELECT \n",
    "    ds.*,\n",
    "    CASE WHEN lc.mql_id IS NOT NULL THEN 1 ELSE 0 END AS promo_flag\n",
    "FROM daily_sales_lag ds\n",
    "LEFT JOIN leads_closed lc \n",
    "  ON ds.product_id = lc.mql_id;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "cursor.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM sales_with_promotions\", conn)\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"sales_with_promotions.csv\", index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1d695",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "        Flags days with potential promotions for demand forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685ecbe",
   "metadata": {},
   "source": [
    "5️⃣ Compute safety stock and reorder points:\n",
    "\n",
    "        Assuming you know lead time (LT) in days and target service level z:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a0b42",
   "metadata": {},
   "source": [
    "Define a user-defined function (UDF) in SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ab189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "\n",
    "# Load daily_sales into pandas\n",
    "df = pd.read_sql_query(\"SELECT * FROM daily_sales\", conn)\n",
    "\n",
    "# Compute mean and std per product + warehouse\n",
    "demand_stats = df.groupby(['product_id', 'warehouse_id'])['units_sold'].agg(\n",
    "    avg_daily_demand='mean',\n",
    "    std_daily_demand='std'\n",
    ").reset_index()\n",
    "\n",
    "# Optional: save back to SQLite\n",
    "demand_stats.to_sql(\"demand_stats\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(demand_stats.head())\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968e623",
   "metadata": {},
   "source": [
    "Safety stock and reorder point calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d50c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "-- Example: Safety stock = z * σ * sqrt(LT), Reorder point = mean*LT + SS\n",
    "-- Assuming z=1.65 (95% service level), LT in days = 7\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS reorder_points AS\n",
    "SELECT \n",
    "    ds.product_id,\n",
    "    ds.warehouse_id,\n",
    "    avg_daily_demand,\n",
    "    std_daily_demand,\n",
    "    ROUND(1.65 * std_daily_demand * SQRT(7),0) AS safety_stock,\n",
    "    ROUND(avg_daily_demand * 7 + 1.65 * std_daily_demand * SQRT(7),0) AS reorder_point\n",
    "FROM demand_stats ds;\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "cursor.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aad6d5",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "        safety_stock accounts for demand variability during lead time.\n",
    "\n",
    "        reorder_point triggers replenishment when inventory reaches this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful if individual product data is sparse.\n",
    "\n",
    "# You can forecast at category × region and then distribute to individual products.\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"olist.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"\"\"\n",
    "-- Aggregate by product category and seller region\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS category_region_sales AS\n",
    "SELECT \n",
    "    p.product_category_name,\n",
    "    s.seller_state AS region,\n",
    "    DATE(o.order_purchase_timestamp) AS sales_date,\n",
    "    SUM(oi.order_item_id) AS units_sold\n",
    "FROM order_items oi\n",
    "JOIN orders o ON oi.order_id = o.order_id\n",
    "JOIN products p ON oi.product_id = p.product_id\n",
    "JOIN sellers s ON oi.seller_id = s.seller_id\n",
    "WHERE o.order_delivered_customer_date IS NOT NULL\n",
    "GROUP BY product_category_name, region, sales_date;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "cursor.execute(sql)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181853c0",
   "metadata": {},
   "source": [
    "Forecasting: export prepared tables to Python / R / ML frameworks:\n",
    "\n",
    "        Prophet: seasonality & holidays\n",
    "\n",
    "        SARIMA: stationary time series\n",
    "\n",
    "        XGBoost/LightGBM: cross-sectional ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
